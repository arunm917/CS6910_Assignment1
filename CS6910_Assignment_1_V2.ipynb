{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arunm917/CS6910_Assignment1/blob/main/CS6910_Assignment_1_V2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VeF0xApq7oQW"
      },
      "outputs": [],
      "source": [
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Aqal939HNhx"
      },
      "source": [
        "# Importing packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "3iv-dm8IHFiD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gdown\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error\n",
        "from tqdm.notebook import trange, tqdm\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "# import tensorflow as ts\n",
        "from tensorflow import keras\n",
        "from sklearn.metrics import accuracy_score, log_loss\n",
        "import wandb\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbXfiEAaDi6c"
      },
      "outputs": [],
      "source": [
        "wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RH_78eMoUz_"
      },
      "source": [
        "# Data preprocessing - fashion MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9o4nbueocl5",
        "outputId": "07967057-f1d6-49a1-ba10-79109972086d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "(X_train, Y_train), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "q42AW6dqE3jt"
      },
      "outputs": [],
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X_train, Y_train, stratify = Y_train, random_state=7, test_size = 0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "1lOnsGYSq0YV"
      },
      "outputs": [],
      "source": [
        "X_train_scaled = X_train.reshape(len(X_train),28*28)/255.0\n",
        "X_val_scaled = X_val.reshape(len(X_val),28*28)/255.0\n",
        "X_test_scaled = X_test.reshape(len(X_test),28*28)/255.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCN_Xp6OHz8u"
      },
      "source": [
        "Encoding labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHgfHPrWHNJ_",
        "outputId": "a6c16abd-0208-4966-97d9-6cd07aedf217"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(54000, 10) (6000, 10) (10000, 10)\n"
          ]
        }
      ],
      "source": [
        "enc = OneHotEncoder()\n",
        "y_train_enc = enc.fit_transform(np.expand_dims(y_train,1)).toarray()\n",
        "y_val_enc = enc.fit_transform(np.expand_dims(y_val,1)).toarray()\n",
        "y_test_enc = enc.fit_transform(np.expand_dims(y_test,1)).toarray()\n",
        "print(y_train_enc.shape, y_val_enc.shape, y_test_enc.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWHhmz9InIjr"
      },
      "source": [
        "Batching"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "m8f50PPnnHm6"
      },
      "outputs": [],
      "source": [
        "X_train_batch = X_train_scaled[0:1000,:]\n",
        "y_train_batch = y_train_enc[0:1000,:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q60obFRBo23w",
        "outputId": "79cbe67c-dc55-4cae-fb9f-2bf7e133bb39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(54000, 784) (54000, 10) (1000, 784) (1000, 10)\n"
          ]
        }
      ],
      "source": [
        "print(X_train_scaled.shape, y_train_enc.shape, X_train_batch.shape, y_train_batch.shape )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9mmePvZHTlv"
      },
      "source": [
        "# Initializations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "uoZUDSWvHcSw"
      },
      "outputs": [],
      "source": [
        "class WeightInitialization:\n",
        "  \n",
        "  def initialize(self, sizes, initialization):\n",
        "\n",
        "    self.W = {}\n",
        "    self.B = {}\n",
        "    self.sizes = sizes\n",
        "    self.nh = len(self.sizes)-2\n",
        "\n",
        "    if initialization =='RANDOM':                       # Random initialization\n",
        "      for i in range(self.nh + 1):\n",
        "        self.W[i+1] = np.random.randn(self.sizes[i+1], self.sizes[i])\n",
        "        self.B[i+1] = np.zeros((self.sizes[i+1],1))\n",
        "\n",
        "      return(self.W, self.B)\n",
        "  \n",
        "    if initialization == 'XAVIER':                      # Xavier initialization\n",
        "      for i in range(self.nh + 1):\n",
        "        std_dev_xavier = np.sqrt(2.0 / (self.sizes[i] + self.sizes[i+1]))\n",
        "        self.W[i+1] = np.random.normal(loc=0, scale=std_dev_xavier, size=(self.sizes[i+1], self.sizes[i]))\n",
        "        self.B[i+1] = np.zeros((self.sizes[i+1],1))\n",
        "\n",
        "      return(self.W, self.B)\n",
        "\n",
        "    if initialization == 'HE':                         # He initialization\n",
        "      print('HE')\n",
        "      for i in range(self.nh + 1):\n",
        "        std_dev_he = np.sqrt(2.0 / self.sizes[i])\n",
        "        self.W[i+1] = np.random.randn(self.sizes[i+1], self.sizes[i]) * std_dev_he\n",
        "        self.B[i+1] = np.zeros((self.sizes[i+1],1))\n",
        "\n",
        "      return(self.W, self.B)\n",
        "    \n",
        "initial = WeightInitialization()  #Instantiation for the class initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQAFW9LhIHSv"
      },
      "source": [
        "# Activation functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "gaYebnsAIJXs"
      },
      "outputs": [],
      "source": [
        "class ActivationFunctions:\n",
        "\n",
        "  def activation(self, z, activation_function):\n",
        "    if activation_function == 'SIGMOID':\n",
        "      return 1/(1+np.exp(-z))\n",
        "    if activation_function == 'TANH':\n",
        "      return np.tanh(z)\n",
        "    if activation_function == 'RELU':\n",
        "      return np.maximum(0,z.T)\n",
        "\n",
        "  def softmax(self, z):\n",
        "    return np.exp(z)/np.sum(np.exp(z))\n",
        "\n",
        "  def der_activation(self, z, activation_function):\n",
        "    if activation_function == 'SIGMOID':\n",
        "      return z*(1-z)\n",
        "    if activation_function == 'TANH':\n",
        "      return (1 - z**2)\n",
        "    if activation_function == 'RELU':\n",
        "      return np.where(z.T > 0, 1, 0)\n",
        "\n",
        "act = ActivationFunctions()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iklJTyvIPQI"
      },
      "source": [
        "# Loss Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "Ysm6kTMgIS42"
      },
      "outputs": [],
      "source": [
        "class LossFunctions:\n",
        "  def MSE(self, y, y_pred):\n",
        "    return (np.sum((y-y_pred)**2))/len(y)\n",
        "  \n",
        "  def cross_entropy(self, y, y_pred):\n",
        "    # clipping y_pred to prevent log(0) errors\n",
        "    #print(y.shape,y_pred.shape)\n",
        "    y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
        "    ce_loss = np.mean(np.sum(np.multiply(-y,np.log(y_pred)), axis = 0))\n",
        "    #print(ce_loss)\n",
        "    return(ce_loss)\n",
        "loss_functions = LossFunctions()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "km6C0oq-IyOP"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "m78pqHeerZZ3"
      },
      "outputs": [],
      "source": [
        "class Model():\n",
        "    \n",
        "  def forward_prop(self, x, activation_function):\n",
        "    self.A = {}\n",
        "    self.H = {}\n",
        "    self.x = x\n",
        "    loss_batch = 0\n",
        "    self.H[0] = self.x.T\n",
        "\n",
        "    for i in range(self.nh + 1):\n",
        "      self.A[i+1] = np.matmul(self.W[i+1], self.H[i]) + self.B[i+1]\n",
        "      self.H[i+1] = act.activation(self.A[i+1], activation_function)\n",
        "\n",
        "    self.y_pred = act.softmax(self.A[i+1])\n",
        "    #print(y_pred)\n",
        "    #print('Y = ', self.y.shape, 'Y_pred =', y_pred.shape)\n",
        "    return(self.y_pred)\n",
        "  \n",
        "  def grad(self, x, y, activation_function, W, B):\n",
        "\n",
        "    if W is None:\n",
        "      W = self.W\n",
        "    if B is None:\n",
        "      B = self.B\n",
        "\n",
        "    y_pred = self.forward_prop(x, activation_function)\n",
        "    self.dW = {}\n",
        "    self.dB = {}\n",
        "    self.dA = {}\n",
        "    self.dH = {}\n",
        "    self.dA[self.nh + 1] = y_pred - y.T\n",
        "\n",
        "    for i in range(self.nh + 1 , 0, -1):\n",
        "      self.dW[i]      = np.matmul(self.dA[i],self.H[i-1].T)\n",
        "      self.dB[i]      = np.sum(self.dA[i], axis = 1).reshape(-1,1)\n",
        "      self.dH[i-1]    = np.matmul(W[i].T,self.dA[i])\n",
        "      self.dA[i-1]    = np.multiply(self.dH[i-1], act.der_activation(self.H[i-1], activation_function))\n",
        "    return (self.dW, self.dB)\n",
        "  \n",
        "  def fit(self, X, Y, X_val, Y_val, epochs, learning_rate, hidden_layers, neurons_per_layer, batch_size, optimizer, initialization, activation_function):\n",
        "    \n",
        "    # x = X[0:batch_size,:]\n",
        "    # y = Y[0:batch_size,:]\n",
        "    #self.X = X\n",
        "    self.loss_epoc_store = []\n",
        "    self.nx = X.shape[1]\n",
        "    self.ny = Y.shape[1]\n",
        "    self.nh = hidden_layers\n",
        "    self.neurons = neurons_per_layer\n",
        "    hidden_layer_sizes = [self.neurons]*self.nh\n",
        "    self.sizes = [self.nx] + hidden_layer_sizes + [self.ny]\n",
        "    print(self.sizes)\n",
        "    if optimizer == 'SGD':\n",
        "      batch_size =1\n",
        "\n",
        "    step_size = len(X)/batch_size\n",
        "\n",
        "    self.W, self.B = initial.initialize(self.sizes, initialization)\n",
        "    opt = Optimizer(self.W, self.B, self.sizes, batch_size, learning_rate, optimizer)\n",
        "\n",
        "    for i in trange(epochs, total=epochs, unit=\"epoch\"):\n",
        "      epoch = i+1\n",
        "      step = 0\n",
        "      # print('epoch = ', epoch)\n",
        "      loss_epoc = 0\n",
        "      self.loss_batch_store = []\n",
        "\n",
        "      for i in range(0,len(X),batch_size):\n",
        "        step += 1\n",
        "        # print('step = ', step)\n",
        "        self.x = X[i:i+batch_size,:]\n",
        "        self.y = Y[i:i+batch_size,:]\n",
        "        #print('y =', self.y.shape)\n",
        "        (self.dW, self.dB) = self.grad(self.x, self.y, activation_function, W = None, B = None)\n",
        "        self.W, self.B = opt.learning_algoithms(self.x, self.y, self.dW, self.dB, step)\n",
        "        # Predicting loss for each batch\n",
        "        loss_batch = loss_functions.cross_entropy(self.y.T ,self.y_pred)\n",
        "        self.loss_batch_store.append(loss_batch)\n",
        "\n",
        "      loss_epoc = np.sum(self.loss_batch_store)/step_size\n",
        "      print('training loss = ', round(loss_epoc,4))\n",
        "      self.loss_epoc_store.append(loss_epoc)\n",
        "\n",
        "      # Predicting validation loss\n",
        "      y_pred_val = self.forward_prop(X_val, activation_function)\n",
        "      Y_pred_val = np.array(y_pred_val.T).squeeze()\n",
        "      Y_pred_val = np.argmax(Y_pred_val,1)\n",
        "      #print(Y_pred_val[0:10], Y_val[0:10])\n",
        "      accuracy_val = accuracy_score(Y_pred_val,Y_val)\n",
        "      print('validation accuracy = ', round(accuracy_val,2))\n",
        "    \n",
        "    plt.plot(self.loss_epoc_store)\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('log_loss')\n",
        "    plt.show()\n",
        "\n",
        "model = Model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQGzzm1ZIVsP"
      },
      "source": [
        "# Learning Algorithms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ps_e-AyMIYzR"
      },
      "outputs": [],
      "source": [
        "class Optimizer:\n",
        "  def __init__(self,W, B, sizes, batch_size, learning_rate, optimizer):\n",
        "    self.W = W\n",
        "    self.B = B\n",
        "    # self.dW = dW\n",
        "    # self.dB = dB\n",
        "    self.sizes = sizes\n",
        "    self.batch_size = batch_size\n",
        "    self.learning_rate = learning_rate\n",
        "    self.optimizer = optimizer\n",
        "    # self.epoch = epoch\n",
        "    # self.step = step\n",
        "    self.nh = len(self.sizes) - 2\n",
        "    self.v_w = {}\n",
        "    self.v_b = {}\n",
        "    self.m_w = {}\n",
        "    self.m_b = {}\n",
        "    #Initializing\n",
        "    for i in range(self.nh + 1):\n",
        "      self.v_w[i+1] = np.zeros((self.sizes[i+1], self.sizes[i]))\n",
        "      self.v_b[i+1] = np.zeros((self.sizes[i+1],1))\n",
        "      self.m_w[i+1] = np.zeros((self.sizes[i+1], self.sizes[i]))\n",
        "      self.m_b[i+1] = np.zeros((self.sizes[i+1],1))\n",
        "\n",
        "  def learning_algoithms(self,x, y, dW, dB, update):\n",
        "\n",
        "    if self.optimizer == 'SGD':         # Stochastic gradient descent\n",
        "      for i in range(self.nh + 1):\n",
        "        self.W[i+1] -= self.learning_rate * (dW[i+1])\n",
        "        self.B[i+1] -= self.learning_rate * (dB[i+1])\n",
        "\n",
        "    if self.optimizer == 'MBGD':        # Mini-batch gradient descent\n",
        "      for i in range(self.nh + 1):\n",
        "        self.W[i+1] -= self.learning_rate * (dW[i+1]/self.batch_size)\n",
        "        self.B[i+1] -= self.learning_rate * (dB[i+1]/self.batch_size)\n",
        "\n",
        "    if self.optimizer == 'MGD':         # Momentum-based gradient descent\n",
        "      beta = 0.9\n",
        "      for i in range(self.nh + 1):\n",
        "        # Updating history term\n",
        "        self.v_w[i+1] = beta*self.v_w[i+1] + self.learning_rate * (dW[i+1]/self.batch_size)\n",
        "        self.v_b[i+1] = beta*self.v_b[i+1] + self.learning_rate * (dB[i+1]/self.batch_size)\n",
        "        # Updating weights and biases\n",
        "        self.W[i+1] -= self.v_w[i+1]\n",
        "        self.B[i+1] -= self.v_b[i+1]\n",
        "\n",
        "    if self.optimizer == 'NAG':          # Nestrov accelarated gradient descent\n",
        "      beta = 0.9\n",
        "      for i in range(self.nh + 1):\n",
        "        # Computing look ahead term\n",
        "        self.v_w[i+1] = beta*self.v_w[i+1]\n",
        "        self.v_b[i+1] = beta*self.v_b[i+1]\n",
        "        self.W[i+1] = self.W[i+1] - self.v_w[i+1]\n",
        "        self.B[i+1] = self.B[i+1] - self.v_b[i+1]\n",
        "      (dW, dB) = model.grad(x, y, self.W, self.B)\n",
        "        # Updating weights and biases\n",
        "      for i in range(self.nh + 1):\n",
        "        # Updating history term\n",
        "        self.v_w[i+1] = beta*self.v_w[i+1] + self.learning_rate * (dW[i+1]/self.batch_size)\n",
        "        self.v_b[i+1] = beta*self.v_b[i+1] + self.learning_rate * (dB[i+1]/self.batch_size)\n",
        "        # Updating weights and biases\n",
        "        self.W[i+1] -= self.v_w[i+1]\n",
        "        self.B[i+1] -= self.v_b[i+1]\n",
        "\n",
        "    if self.optimizer == 'RMSPROP':       # Root mean squared propagation\n",
        "      beta = 0.9\n",
        "      eps = 1e-8\n",
        "      for i in range(self.nh + 1):\n",
        "        # Updating history term\n",
        "        self.v_w[i+1] = beta*self.v_w[i+1] + (1-beta) * ((dW[i+1]/self.batch_size)**2)\n",
        "        self.v_b[i+1] = beta*self.v_b[i+1] + (1-beta) * ((dB[i+1]/self.batch_size)**2)\n",
        "        # Updating weights and biases\n",
        "        self.W[i+1] -= (self.learning_rate/np.sqrt(self.v_w[i+1] + eps)) * (dW[i+1]/self.batch_size)\n",
        "        self.B[i+1] -= (self.learning_rate/np.sqrt(self.v_b[i+1] + eps)) * (dB[i+1]/self.batch_size)\n",
        "\n",
        "    if self.optimizer == 'ADAM':           # Adaptive moment estimation\n",
        "      beta1 = 0.9\n",
        "      beta2 = 0.999\n",
        "      eps = 1e-8\n",
        "      for i in range(self.nh + 1):\n",
        "\n",
        "        # Updating history term\n",
        "        self.m_w[i+1] = beta1*self.m_w[i+1] + (1-beta1) * (dW[i+1]/self.batch_size)\n",
        "        self.m_b[i+1] = beta1*self.m_b[i+1] + (1-beta1) * (dB[i+1]/self.batch_size)\n",
        "\n",
        "        m_w_hat = self.m_w[i+1]/(1 - np.power(beta1,update))\n",
        "        m_b_hat = self.m_b[i+1]/(1 - np.power(beta1,update))\n",
        "\n",
        "        self.v_w[i+1] = beta2*self.v_w[i+1] + (1-beta2) * ((dW[i+1]/self.batch_size)**2)\n",
        "        self.v_b[i+1] = beta2*self.v_b[i+1] + (1-beta2) * ((dB[i+1]/self.batch_size)**2)\n",
        "\n",
        "        v_w_hat = self.v_w[i+1]/(1 - np.power(beta2,update))\n",
        "        v_b_hat = self.v_b[i+1]/(1 - np.power(beta2,update))\n",
        "\n",
        "        # Updating weights and biases\n",
        "        self.W[i+1] -= (self.learning_rate/(np.sqrt(self.v_w[i+1]) + eps)) * m_w_hat\n",
        "        self.B[i+1] -= (self.learning_rate/(np.sqrt(self.v_b[i+1]) + eps)) * m_b_hat\n",
        "\n",
        "    if self.optimizer == 'NADAM':          # Nestrov Adaptive moment estimation\n",
        "      beta1 = 0.9\n",
        "      beta2 = 0.999\n",
        "      eps = 1e-8\n",
        "      for i in range(self.nh + 1):\n",
        "\n",
        "        # Updating history term\n",
        "        self.m_w[i+1] = beta1*self.m_w[i+1] + (1-beta1) * (dW[i+1]/self.batch_size)\n",
        "        self.m_b[i+1] = beta1*self.m_b[i+1] + (1-beta1) * (dB[i+1]/self.batch_size)\n",
        "\n",
        "        m_w_hat = self.m_w[i+1]/(1 - np.power(beta1,update))\n",
        "        m_b_hat = self.m_b[i+1]/(1 - np.power(beta1,update))\n",
        "\n",
        "        self.v_w[i+1] = beta2*self.v_w[i+1] + (1-beta2) * ((dW[i+1]/self.batch_size)**2)\n",
        "        self.v_b[i+1] = beta2*self.v_b[i+1] + (1-beta2) * ((dB[i+1]/self.batch_size)**2)\n",
        "\n",
        "        v_w_hat = self.v_w[i+1]/(1 - np.power(beta2,update))\n",
        "        v_b_hat = self.v_b[i+1]/(1 - np.power(beta2,update))\n",
        "\n",
        "        # Updating weights and biases\n",
        "        self.W[i+1] -= (self.learning_rate/(np.sqrt(self.v_w[i+1]) + eps)) * (m_w_hat*beta1 + (((1-beta1)*dW[i+1])/(1-np.power(beta1,update))))\n",
        "        self.B[i+1] -= (self.learning_rate/(np.sqrt(self.v_b[i+1]) + eps)) * (m_b_hat*beta1 + (((1-beta1)*dB[i+1])/(1-np.power(beta1,update))))\n",
        "    \n",
        "    return(self.W, self.B)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOwz4bgsRIii"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wqISZGEjvyoP"
      },
      "outputs": [],
      "source": [
        "model.fit(X = X_train_scaled, \n",
        "          Y = y_train_enc,\n",
        "          X_val = X_val_scaled,\n",
        "          Y_val = y_val,\n",
        "          epochs = 20, \n",
        "          learning_rate = 0.0001,\n",
        "          hidden_layers = 2,\n",
        "          neurons_per_layer = 64,\n",
        "          batch_size = 32,\n",
        "          optimizer = 'NADAM',\n",
        "          initialization = 'HE',\n",
        "          activation_function = 'TANH')\n",
        "# For SGD batch-size is automatically set to 1"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "_Aqal939HNhx",
        "5RH_78eMoUz_",
        "ZQGzzm1ZIVsP"
      ],
      "provenance": [],
      "authorship_tag": "ABX9TyMXh4gvStZt+03kL8P7ocaG",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}